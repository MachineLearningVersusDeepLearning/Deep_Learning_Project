{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_basis.ipynb",
      "provenance": [],
      "mount_file_id": "1pvJmeQz7AwaHzUgDcDmei8GBfgd_lReO",
      "authorship_tag": "ABX9TyMKUFybKYHNkno6NrZ4bnFy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MachineLearningVersusDeepLearning/Deep_Learning_Project/blob/master/03.Deep_Learning_All_In_One_Fast_Campus/05_deep_learning_latest_trend/GAN_basis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjHT9M6X9Rsh"
      },
      "source": [
        "# DCGAN 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUTqfISyR5c7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmj0x1dlTeT0",
        "outputId": "11a9474f-810e-4372-86f5-73438f789615"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu6AACfYR5uL",
        "outputId": "3ac92d92-034c-464d-ec3c-da69c9ba5457"
      },
      "source": [
        "!git clone https://github.com/Machi`neLearningVersusDeepLearning/ZeroToAll.git '/gdrive/My Drive/Colab Notebooks/05_deep_learning_state_of_the_art_trend/GAN'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching ``'\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6xV67KTZQVM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhoUpYxs9cBs"
      },
      "source": [
        "# Library 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2o45rnK61Rj"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, Layer, Conv2D, BatchNormalization, Flatten, Conv2DTranspose\n",
        " \n",
        " \n",
        "######## 해결 - 일반적으로 model을 쌓지 않고 class로 할때는 call 함수를 사용하여 구현함 ##############\n",
        "class Generator(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ########### 왜 7*7*512 인 것인가 #################\n",
        "        self.dense = Dense(7*7*512, use_bias=False, input_shape=(100,))\n",
        "        \n",
        "        ########### batch normalization 이란 무엇인가 ########### \n",
        "        self.bn = BatchNormalization()\n",
        "        self.lrelu = LeakyReLU()\n",
        "        \n",
        "        ########## conv2dtranspose 란 무엇인가 ###################\n",
        "        self.deconv1 = Conv2DTranspose(256, (5, 5), strides=(1, 1), padding='same', use_bias=False)\n",
        "        self.bn1 = BatchNormalization()\n",
        "        self.lrelu1 = LeakyReLU()\n",
        "        \n",
        "        self.deconv2 = Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False)\n",
        "        self.bn2 = BatchNormalization()\n",
        "        self.lrelu2 = LeakyReLU()\n",
        "        \n",
        "        self.deconv3 = Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same')\n",
        " \n",
        "    def call(self, x, training=None, mask=None):\n",
        "        h = self.lrelu(self.bn(self.dense(x), training))\n",
        "        ########### 왜 reshape을 더 먼저하지 않았는가 ? ############\n",
        "        h = tf.reshape(h, (-1, 7, 7, 512))\n",
        "        h = self.lrelu1(self.bn1(self.deconv1(h), training))\n",
        "        h = self.lrelu2(self.bn2(self.deconv2(h), training))\n",
        "        return self.deconv3(h)\n",
        " \n",
        "class Discriminator(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv2D(64, (5, 5), strides=(2, 2), padding='same')\n",
        "        self.bn1 = BatchNormalization()\n",
        "        self.lrelu1 = LeakyReLU()\n",
        "        \n",
        "        self.conv2 = Conv2D(128, (5, 5), strides=(2, 2), padding='same')\n",
        "        self.bn2 = BatchNormalization()\n",
        "        self.lrelu2 = LeakyReLU()\n",
        "        \n",
        "        self.flatten = Flatten()\n",
        "        self.dense = Dense(1, activation='sigmoid')\n",
        " \n",
        "    def call(self, x, training=None, mask=None):\n",
        "        h = self.lrelu1(self.bn1(self.conv1(x), training))\n",
        "        h = self.lrelu2(self.bn2(self.conv2(h), training))\n",
        "        return self.dense(h)\n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        "######## 데이터 불러오기 ########\n",
        " \n",
        "dataset = tfds.load(name='fashion_mnist', split=tfds.Split.TRAIN)\n",
        "dataset = dataset.map(lambda x: tf.cast(x['image'], tf.float32) / 255.0).batch(32)\n",
        " \n",
        "####### 모델 최적화 ########\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        " \n",
        "optim_d = tf.optimizers.Adam(1e-4)\n",
        "optim_g = tf.optimizers.Adam(1e-4)\n",
        " \n",
        "d_mean = tf.metrics.Mean()\n",
        "g_mean = tf.metrics.Mean()\n",
        " \n",
        "########### 로스 선언 #########\n",
        "def discriminator_loss(d_real, d_fake):\n",
        "    real_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(d_real), d_real)\n",
        "    fake_loss = tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(d_fake), d_fake)\n",
        "    return real_loss + fake_loss\n",
        " \n",
        "def generator_loss(d_fake):\n",
        "    return tf.keras.losses.BinaryCrossentropy()(tf.ones_like(d_fake), d_fake)\n",
        " \n",
        " \n",
        " \n",
        " \n",
        "########## 학습 #############\n",
        "# @tf.function이란 무엇인가 ?? \n",
        "@tf.function\n",
        "def train_step(image, optim_d, optim_g):\n",
        "    ######## gradient tape이란 무엇인가 ? ##########\n",
        "    with tf.GradientTape() as tape_d, tf.GradientTape() as tape_g: \n",
        "        z = tf.random.normal([32, 100])\n",
        "        g = generator(z, training=True)\n",
        "        \n",
        "        d_real = discriminator(image, training=True)\n",
        "        d_fake = discriminator(g, training=True)\n",
        "        \n",
        "        d_loss = discriminator_loss(d_real, d_fake)\n",
        "        g_loss = generator_loss(d_fake)\n",
        "        \n",
        "        gradients_d = tape_d.gradient(d_loss, discriminator.trainable_weights)\n",
        "        gradients_g = tape_g.gradient(g_loss, generator.trainable_weights)\n",
        "    \n",
        "    optim_d.apply_gradients(zip(gradients_d, discriminator.trainable_weights))\n",
        "    optim_g.apply_gradients(zip(gradients_g, generator.trainable_weights))\n",
        "    return d_loss, g_loss\n",
        " \n",
        "######### 학습 시작 #############\n",
        "sample_z = tf.random.normal([10, 100])\n",
        "for epoch in range(100):\n",
        "    for image in dataset:\n",
        "        d_loss, g_loss = train_step(image, optim_d, optim_g)\n",
        " \n",
        "        d_mean.update_state(d_loss)\n",
        "        g_mean.update_state(g_loss)\n",
        " \n",
        "    print('epoch: {}, d_loss: {}, g_loss: {}'.format(epoch+1, d_mean.result(), g_mean.result()))\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        img_list = list()\n",
        "        sample_img = generator(sample_z)\n",
        "        for idx in range(sample_img.shape[0]):\n",
        "            img_list.append(sample_img[idx][:, :, 0])\n",
        "        img = np.concatenate(img_list, axis=1)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.show()\n",
        " \n",
        "    d_mean.reset_states()\n",
        "    g_mean.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}